{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceLength = 10\n",
    "sourceTextFileName = \"poem.txt\"\n",
    "trainingTextFileName = \"train_\" + sourceTextFileName\n",
    "mappingFileName = \"SimpleNLP_mapping.pkl\"\n",
    "bestSavedModel = \"bestSavedSimpleNLPModel.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads and return the text of the training file. self-explanatory\n",
    "def readTextFile(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTextFile(lines, fileName):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(fileName, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRawText(rawText):\n",
    "    # split the raw text using space (' ') \n",
    "    tokens = rawText.split()\n",
    "    rawText = ' '.join(tokens)\n",
    "    # basically we removed all the line/paragraph breaks\n",
    "    # but kept the punctuations\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequenceRawText(rawText):\n",
    "    # organize into sequences of characters\n",
    "    length = 10\n",
    "    sequences = list()\n",
    "    for i in range(length, len(rawText)):\n",
    "        # picks a sequence of tokens\n",
    "        seq = rawText[i - length:i+1]\n",
    "        # add to tlist\n",
    "        sequences.append(seq)\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence,\n",
      "A pocket full of rye.\n",
      "Four and twenty blackbirds,\n",
      "Baked in a pie.\n",
      "\n",
      "When the pie was opened\n",
      "The birds began to sing;\n"
     ]
    }
   ],
   "source": [
    "# load the training file\n",
    "rawText = readTextFile(sourceTextFileName)\n",
    "print(rawText[: 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence, A pocket full of rye. Four and twenty blackbirds, Baked in a pie. When the pie was opened The birds began to sing; Wasn’t that a dainty dish, To set before the king. The king was in his counting house, Counting out his money; The queen was in the parlour, Eating bread and honey. The maid was in the garden, Hanging out the clothes, When down came a blackbird And pecked off her nose.\n"
     ]
    }
   ],
   "source": [
    "rawText = processRawText(rawText)\n",
    "print(rawText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences: 399\n",
      "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ']\n"
     ]
    }
   ],
   "source": [
    "# turn the text into a sequence of character\n",
    "# each sequence is sequenceLength long\n",
    "sequences = sequenceRawText(rawText)\n",
    "\n",
    "# save sequences to file\n",
    "writeTextFile(sequences, trainingTextFileName)\n",
    "\n",
    "print('Total number of sequences: %d' % len(sequences))\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ']\n"
     ]
    }
   ],
   "source": [
    "# now we read the text sequences from the file we saved\n",
    "rawTrainingText = readTextFile(trainingTextFileName)\n",
    "sequenceLines = rawTrainingText.split('\\n')\n",
    "\n",
    "print(sequenceLines[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 38\n",
      "['\\n', ' ', ',', '.', ';', 'A', 'B', 'C', 'E', 'F', 'H', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'x', 'y', '’']\n",
      "{'\\n': 0, ' ': 1, ',': 2, '.': 3, ';': 4, 'A': 5, 'B': 6, 'C': 7, 'E': 8, 'F': 9, 'H': 10, 'S': 11, 'T': 12, 'W': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'w': 34, 'x': 35, 'y': 36, '’': 37}\n"
     ]
    }
   ],
   "source": [
    "# time to encode the text\n",
    "\n",
    "# create a vocabulary (all used characters in the text)\n",
    "vocab = sorted(list(set(rawTrainingText)))\n",
    "vocabSize = len(vocab)\n",
    "\n",
    "# map each character to an integer by creating a dictionary\n",
    "vocabMap = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "print(\"Vocabulary size: %d\" % vocabSize)\n",
    "print(vocab)\n",
    "print(vocabMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 22, 26, 20, 1, 14, 1, 31, 27, 26, 20], [22, 26, 20, 1, 14, 1, 31, 27, 26, 20, 1], [26, 20, 1, 14, 1, 31, 27, 26, 20, 1, 27], [20, 1, 14, 1, 31, 27, 26, 20, 1, 27, 19], [1, 14, 1, 31, 27, 26, 20, 1, 27, 19, 1]]\n"
     ]
    }
   ],
   "source": [
    "# let's turn those characters in the sequences to integers \n",
    "sequences = list()\n",
    "\n",
    "for line in sequenceLines:\n",
    "    # encode line\n",
    "    encodedSequence = [vocabMap[char] for char in line]\n",
    "    # add it to the list\n",
    "    sequences.append(encodedSequence)\n",
    "    \n",
    "print(sequences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 22 26 20  1 14  1 31 27 26]\n",
      " [22 26 20  1 14  1 31 27 26 20]\n",
      " [26 20  1 14  1 31 27 26 20  1]\n",
      " [20  1 14  1 31 27 26 20  1 27]\n",
      " [ 1 14  1 31 27 26 20  1 27 19]]\n",
      "[20  1 27 19  1]\n"
     ]
    }
   ],
   "source": [
    "# now we need to prepare the input and target matrices\n",
    "\n",
    "# X = sequences[:,:-1] means we are grabbing all the rows from sequences but dropping the last column\n",
    "# .... the last column will be used as the target\n",
    "\n",
    "# y = sequences[:,-1] means we are grabbing all the rows from sequences but only retaining the last column \n",
    "# .... the last column being our target\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last thing...\n",
    "# we need to one-hot-encode each character. \n",
    "# That is, each input vector (of sequenceLength) becomes a vector as long as the vocabulary\n",
    "# with a 1 marked for the specific character. \n",
    "\n",
    "# for this we use the to_categorical() function in the Keras API to one-hot-encode\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocabSize) for x in X]\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially each input becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, sequenceLength, vocabSize)\n",
    "\n",
    "# and each output becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 10, 38)\n",
      "(399, 38)\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 75)                34200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                2888      \n",
      "=================================================================\n",
      "Total params: 37,088\n",
      "Trainable params: 37,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# time to model...\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(vocabSize, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batchSize = 20\n",
    "optimizerFunction = \"adam\"\n",
    "# optimizerFunction = \"rmsprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 1.4223 - acc: 0.6216\n",
      "Epoch 2/100\n",
      " - 1s - loss: 1.3394 - acc: 0.6742\n",
      "Epoch 3/100\n",
      " - 1s - loss: 1.3050 - acc: 0.6767\n",
      "Epoch 4/100\n",
      " - 1s - loss: 1.2732 - acc: 0.6792\n",
      "Epoch 5/100\n",
      " - 1s - loss: 1.2371 - acc: 0.6967\n",
      "Epoch 6/100\n",
      " - 1s - loss: 1.2103 - acc: 0.7243\n",
      "Epoch 7/100\n",
      " - 1s - loss: 1.1892 - acc: 0.6992\n",
      "Epoch 8/100\n",
      " - 1s - loss: 1.1640 - acc: 0.7268\n",
      "Epoch 9/100\n",
      " - 1s - loss: 1.1234 - acc: 0.7444\n",
      "Epoch 10/100\n",
      " - 1s - loss: 1.0988 - acc: 0.7368\n",
      "Epoch 11/100\n",
      " - 1s - loss: 1.0730 - acc: 0.7644\n",
      "Epoch 12/100\n",
      " - 1s - loss: 1.0527 - acc: 0.7569\n",
      "Epoch 13/100\n",
      " - 1s - loss: 1.0224 - acc: 0.7644\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.9975 - acc: 0.7920\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.9669 - acc: 0.7995\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.9343 - acc: 0.8020\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.9143 - acc: 0.8120\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.8784 - acc: 0.8446\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.8553 - acc: 0.8296\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.8343 - acc: 0.8396\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.8022 - acc: 0.8571\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.7756 - acc: 0.8872\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.7559 - acc: 0.8772\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.7347 - acc: 0.8972\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.7164 - acc: 0.8972\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.6858 - acc: 0.8997\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.6585 - acc: 0.9073\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.6348 - acc: 0.9098\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.6161 - acc: 0.9148\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.6099 - acc: 0.9173\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.5825 - acc: 0.9248\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.5527 - acc: 0.9323\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.5373 - acc: 0.9524\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.5082 - acc: 0.9424\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.4902 - acc: 0.9599\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.4696 - acc: 0.9599\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.4527 - acc: 0.9649\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.4404 - acc: 0.9774\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.4248 - acc: 0.9699\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.4029 - acc: 0.9724\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.3935 - acc: 0.9774\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.3739 - acc: 0.9825\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.3730 - acc: 0.9749\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.3502 - acc: 0.9825\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.3411 - acc: 0.9875\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.3282 - acc: 0.9875\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.3112 - acc: 0.9799\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.2960 - acc: 0.9900\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.2884 - acc: 0.9850\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.2741 - acc: 0.9900\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.2663 - acc: 0.9850\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.2533 - acc: 0.9875\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.2446 - acc: 0.9925\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.2366 - acc: 0.9925\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.2320 - acc: 0.9900\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.2202 - acc: 0.9925\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2114 - acc: 0.9925\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2067 - acc: 0.9900\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.1997 - acc: 0.9900\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.1922 - acc: 0.9925\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.1833 - acc: 0.9900\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.1841 - acc: 0.9925\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.1746 - acc: 0.9950\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.1624 - acc: 0.9975\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.1601 - acc: 0.9950\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.1530 - acc: 0.9925\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.1498 - acc: 0.9950\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.1427 - acc: 0.9950\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.1413 - acc: 0.9950\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.1349 - acc: 0.9900\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.1283 - acc: 0.9925\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.1252 - acc: 0.9925\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.1209 - acc: 0.9950\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.1181 - acc: 0.9950\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.1150 - acc: 0.9950\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.1123 - acc: 0.9925\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.1148 - acc: 0.9925\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.1077 - acc: 0.9950\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.1025 - acc: 0.9950\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.0998 - acc: 0.9950\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0957 - acc: 0.9950\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.0947 - acc: 0.9950\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0901 - acc: 0.9950\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0890 - acc: 0.9950\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0864 - acc: 0.9900\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0852 - acc: 0.9950\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0823 - acc: 0.9925\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0790 - acc: 0.9925\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0785 - acc: 0.9950\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0768 - acc: 0.9925\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0769 - acc: 0.9900\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0717 - acc: 0.9925\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0718 - acc: 0.9950\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0706 - acc: 0.9950\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0678 - acc: 0.9925\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0654 - acc: 0.9950\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0644 - acc: 0.9950\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0652 - acc: 0.9925\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0624 - acc: 0.9950\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0611 - acc: 0.9925\n"
     ]
    }
   ],
   "source": [
    "#checkpointer = ModelCheckpoint(filepath=bestSavedModel, verbose=1, save_best_only=True)\n",
    "#model.fit(X, y, epochs=epochs, batch_size=batchSize, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy, optimizer=optimizerFunction, metrics=[\"accuracy\"])\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save(bestSavedModel)\n",
    "\n",
    "# save the vocabulary map. We need it for character generation part later.\n",
    "dump(vocabMap, open(mappingFileName, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(bestSavedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
