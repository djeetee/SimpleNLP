{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poem.txt\n",
      "poem-Train.txt\n",
      "poem-SimpleNLPMapping.pkl\n",
      "poem-SimpleNLPModel.hdf5\n",
      "poem-SeedText.txt\n"
     ]
    }
   ],
   "source": [
    "sequenceLength = 10\n",
    "\n",
    "sourceTextFileName = \"poem.txt\"\n",
    "#sourceTextFileName = \"anna.txt\"\n",
    "trainingTextFileName = sourceTextFileName.split(\".\")[0] + \"-Train.txt\"\n",
    "mappingFileName = sourceTextFileName.split(\".\")[0] + \"-SimpleNLPMapping\" + \".pkl\"\n",
    "bestSavedModel = sourceTextFileName.split(\".\")[0] + \"-SimpleNLPModel\" + \".hdf5\"\n",
    "seedTextFileName = sourceTextFileName.split(\".\")[0] + \"-SeedText.txt\"\n",
    "\n",
    "print(sourceTextFileName)\n",
    "print(trainingTextFileName)\n",
    "print(mappingFileName)\n",
    "print(bestSavedModel)\n",
    "print(seedTextFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads and return the text of the training file. self-explanatory\n",
    "def readTextFile(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTextFile(lines, fileName):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(fileName, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRawText(rawText):\n",
    "    # split the raw text using space (' ') \n",
    "    tokens = rawText.split()\n",
    "    rawText = ' '.join(tokens)\n",
    "    \n",
    "    # basically we removed all the line/paragraph breaks\n",
    "    # but kept the punctuations\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequenceRawText(rawText):\n",
    "    # organize into sequences of characters\n",
    "    length = 10\n",
    "    sequences = list()\n",
    "    for i in range(length, len(rawText)):\n",
    "        # picks a sequence of tokens\n",
    "        seq = rawText[i - length:i+1]\n",
    "        # add to tlist\n",
    "        sequences.append(seq)\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence,\n",
      "A pocket full of rye.\n",
      "Four and twenty blackbirds,\n",
      "Baked in a pie.\n",
      "\n",
      "When the pie was opened\n",
      "The birds began to sing;\n"
     ]
    }
   ],
   "source": [
    "# load the training file\n",
    "rawText = readTextFile(sourceTextFileName)\n",
    "print(rawText[: 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing a song of sixpence, A pocket full of rye. Four and twenty blackbirds, Baked in a pie. When the pie was opened The birds began to sing; \n"
     ]
    }
   ],
   "source": [
    "rawText = processRawText(rawText)\n",
    "print(rawText[: 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences: 399\n",
      "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ']\n"
     ]
    }
   ],
   "source": [
    "# turn the text into a sequence of character\n",
    "# each sequence is sequenceLength long\n",
    "sequences = sequenceRawText(rawText)\n",
    "\n",
    "# save sequences to file\n",
    "writeTextFile(sequences, trainingTextFileName)\n",
    "\n",
    "print('Total number of sequences: %d' % len(sequences))\n",
    "print(sequences[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ']\n"
     ]
    }
   ],
   "source": [
    "# now we read the text sequences from the file we saved\n",
    "rawTrainingText = readTextFile(trainingTextFileName)\n",
    "sequenceLines = rawTrainingText.split('\\n')\n",
    "\n",
    "print(sequenceLines[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 38\n",
      "['\\n', ' ', ',', '.', ';', 'A', 'B', 'C', 'E', 'F', 'H', 'S', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'x', 'y', '’']\n",
      "{'\\n': 0, ' ': 1, ',': 2, '.': 3, ';': 4, 'A': 5, 'B': 6, 'C': 7, 'E': 8, 'F': 9, 'H': 10, 'S': 11, 'T': 12, 'W': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'w': 34, 'x': 35, 'y': 36, '’': 37}\n"
     ]
    }
   ],
   "source": [
    "# time to encode the text\n",
    "\n",
    "# create a vocabulary (all used characters in the text)\n",
    "vocab = sorted(list(set(rawTrainingText)))\n",
    "vocabSize = len(vocab)\n",
    "\n",
    "# map each character to an integer by creating a dictionary\n",
    "vocabMap = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "print(\"Vocabulary size: %d\" % vocabSize)\n",
    "print(vocab)\n",
    "print(vocabMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 22, 26, 20, 1, 14, 1, 31, 27, 26, 20], [22, 26, 20, 1, 14, 1, 31, 27, 26, 20, 1], [26, 20, 1, 14, 1, 31, 27, 26, 20, 1, 27], [20, 1, 14, 1, 31, 27, 26, 20, 1, 27, 19], [1, 14, 1, 31, 27, 26, 20, 1, 27, 19, 1]]\n"
     ]
    }
   ],
   "source": [
    "# let's turn those characters in the sequences to integers \n",
    "sequences = list()\n",
    "\n",
    "for line in sequenceLines:\n",
    "    # encode line\n",
    "    encodedSequence = [vocabMap[char] for char in line]\n",
    "    # add it to the list\n",
    "    sequences.append(encodedSequence)\n",
    "    \n",
    "print(sequences[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 22 26 20  1 14  1 31 27 26]\n",
      " [22 26 20  1 14  1 31 27 26 20]\n",
      " [26 20  1 14  1 31 27 26 20  1]\n",
      " [20  1 14  1 31 27 26 20  1 27]\n",
      " [ 1 14  1 31 27 26 20  1 27 19]]\n",
      "[20  1 27 19  1]\n"
     ]
    }
   ],
   "source": [
    "# now we need to prepare the input and target matrices\n",
    "\n",
    "# X = sequences[:,:-1] means we are grabbing all the rows from sequences but dropping the last column\n",
    "# .... the last column will be used as the target\n",
    "\n",
    "# y = sequences[:,-1] means we are grabbing all the rows from sequences but only retaining the last column \n",
    "# .... the last column being our target\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last thing...\n",
    "# we need to one-hot-encode each character. \n",
    "# That is, each input vector (of sequenceLength) becomes a vector as long as the vocabulary\n",
    "# with a 1 marked for the specific character. \n",
    "\n",
    "# for this we use the to_categorical() function in the Keras API to one-hot-encode\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocabSize) for x in X]\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially each input becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, sequenceLength, vocabSize)\n",
    "\n",
    "# and each output becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 10, 38)\n",
      "(399, 38)\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.]]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batchSize = 20\n",
    "dropOutRate = 0.3\n",
    "lstmCellNumber = 100\n",
    "\n",
    "activationFunction = \"softmax\"\n",
    "\n",
    "optimizerFunction = \"adam\"\n",
    "# optimizerFunction = \"rmsprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# time to model...\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstmCellNumber, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(dropOutRate))\n",
    "model.add(Dense(vocabSize, activation=activationFunction))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 3.6169 - acc: 0.0727\n",
      "Epoch 2/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 3.4590 - acc: 0.1805\n",
      "Epoch 3/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 3.1370 - acc: 0.1905\n",
      "Epoch 4/100\n",
      "399/399 [==============================] - 0s 888us/step - loss: 3.0785 - acc: 0.1930\n",
      "Epoch 5/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 3.0472 - acc: 0.1679\n",
      "Epoch 6/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 3.0118 - acc: 0.1930\n",
      "Epoch 7/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.9998 - acc: 0.1930\n",
      "Epoch 8/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.9837 - acc: 0.1905\n",
      "Epoch 9/100\n",
      "399/399 [==============================] - 0s 986us/step - loss: 2.9790 - acc: 0.1930\n",
      "Epoch 10/100\n",
      "399/399 [==============================] - 0s 942us/step - loss: 2.9682 - acc: 0.1880\n",
      "Epoch 11/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.9479 - acc: 0.1855\n",
      "Epoch 12/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.9144 - acc: 0.1930\n",
      "Epoch 13/100\n",
      "399/399 [==============================] - 0s 935us/step - loss: 2.8880 - acc: 0.1930\n",
      "Epoch 14/100\n",
      "399/399 [==============================] - 0s 732us/step - loss: 2.8714 - acc: 0.2030\n",
      "Epoch 15/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.8369 - acc: 0.2180\n",
      "Epoch 16/100\n",
      "399/399 [==============================] - 0s 951us/step - loss: 2.7555 - acc: 0.2306\n",
      "Epoch 17/100\n",
      "399/399 [==============================] - 0s 817us/step - loss: 2.7376 - acc: 0.2356\n",
      "Epoch 18/100\n",
      "399/399 [==============================] - 0s 868us/step - loss: 2.6911 - acc: 0.2506\n",
      "Epoch 19/100\n",
      "399/399 [==============================] - 0s 989us/step - loss: 2.6495 - acc: 0.2456\n",
      "Epoch 20/100\n",
      "399/399 [==============================] - 0s 963us/step - loss: 2.5850 - acc: 0.2431\n",
      "Epoch 21/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.5126 - acc: 0.2807\n",
      "Epoch 22/100\n",
      "399/399 [==============================] - 0s 801us/step - loss: 2.4880 - acc: 0.2882\n",
      "Epoch 23/100\n",
      "399/399 [==============================] - 0s 822us/step - loss: 2.4544 - acc: 0.2982\n",
      "Epoch 24/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.4063 - acc: 0.3409\n",
      "Epoch 25/100\n",
      "399/399 [==============================] - 0s 957us/step - loss: 2.3243 - acc: 0.3358\n",
      "Epoch 26/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.2907 - acc: 0.3258\n",
      "Epoch 27/100\n",
      "399/399 [==============================] - 0s 710us/step - loss: 2.2696 - acc: 0.3659\n",
      "Epoch 28/100\n",
      "399/399 [==============================] - 0s 974us/step - loss: 2.1981 - acc: 0.3634\n",
      "Epoch 29/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.1478 - acc: 0.3835\n",
      "Epoch 30/100\n",
      "399/399 [==============================] - 0s 862us/step - loss: 2.1255 - acc: 0.4010\n",
      "Epoch 31/100\n",
      "399/399 [==============================] - 0s 927us/step - loss: 2.0843 - acc: 0.4010\n",
      "Epoch 32/100\n",
      "399/399 [==============================] - 0s 952us/step - loss: 2.1020 - acc: 0.4010\n",
      "Epoch 33/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 2.0203 - acc: 0.4261\n",
      "Epoch 34/100\n",
      "399/399 [==============================] - 0s 857us/step - loss: 1.9925 - acc: 0.3960\n",
      "Epoch 35/100\n",
      "399/399 [==============================] - 0s 824us/step - loss: 1.9624 - acc: 0.4185\n",
      "Epoch 36/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.9312 - acc: 0.4336\n",
      "Epoch 37/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.9086 - acc: 0.4236\n",
      "Epoch 38/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.8481 - acc: 0.4712\n",
      "Epoch 39/100\n",
      "399/399 [==============================] - 0s 846us/step - loss: 1.8045 - acc: 0.4812\n",
      "Epoch 40/100\n",
      "399/399 [==============================] - 0s 925us/step - loss: 1.7598 - acc: 0.4812\n",
      "Epoch 41/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.7339 - acc: 0.4812\n",
      "Epoch 42/100\n",
      "399/399 [==============================] - 0s 973us/step - loss: 1.7122 - acc: 0.4737\n",
      "Epoch 43/100\n",
      "399/399 [==============================] - 0s 817us/step - loss: 1.6184 - acc: 0.4837\n",
      "Epoch 44/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.6193 - acc: 0.5238\n",
      "Epoch 45/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.6043 - acc: 0.5163\n",
      "Epoch 46/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.5386 - acc: 0.5840\n",
      "Epoch 47/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.5189 - acc: 0.5489\n",
      "Epoch 48/100\n",
      "399/399 [==============================] - 0s 967us/step - loss: 1.4715 - acc: 0.5664\n",
      "Epoch 49/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.4478 - acc: 0.5514\n",
      "Epoch 50/100\n",
      "399/399 [==============================] - 0s 855us/step - loss: 1.3732 - acc: 0.6216\n",
      "Epoch 51/100\n",
      "399/399 [==============================] - 0s 867us/step - loss: 1.3503 - acc: 0.5865\n",
      "Epoch 52/100\n",
      "399/399 [==============================] - 0s 817us/step - loss: 1.3612 - acc: 0.5714\n",
      "Epoch 53/100\n",
      "399/399 [==============================] - 0s 850us/step - loss: 1.3602 - acc: 0.6015\n",
      "Epoch 54/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.2442 - acc: 0.6742\n",
      "Epoch 55/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.2638 - acc: 0.6165\n",
      "Epoch 56/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.2124 - acc: 0.6591\n",
      "Epoch 57/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.2219 - acc: 0.6441\n",
      "Epoch 58/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.1697 - acc: 0.6842\n",
      "Epoch 59/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.1115 - acc: 0.6992\n",
      "Epoch 60/100\n",
      "399/399 [==============================] - 0s 630us/step - loss: 1.0882 - acc: 0.7168\n",
      "Epoch 61/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.0629 - acc: 0.6817\n",
      "Epoch 62/100\n",
      "399/399 [==============================] - 0s 980us/step - loss: 1.0134 - acc: 0.7293\n",
      "Epoch 63/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 1.0335 - acc: 0.7018\n",
      "Epoch 64/100\n",
      "399/399 [==============================] - 0s 868us/step - loss: 0.9360 - acc: 0.7744\n",
      "Epoch 65/100\n",
      "399/399 [==============================] - 0s 773us/step - loss: 0.9217 - acc: 0.7519\n",
      "Epoch 66/100\n",
      "399/399 [==============================] - 0s 744us/step - loss: 0.8999 - acc: 0.7669\n",
      "Epoch 67/100\n",
      "399/399 [==============================] - 0s 969us/step - loss: 0.9226 - acc: 0.7569\n",
      "Epoch 68/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.8248 - acc: 0.8195\n",
      "Epoch 69/100\n",
      "399/399 [==============================] - 0s 930us/step - loss: 0.8172 - acc: 0.7920\n",
      "Epoch 70/100\n",
      "399/399 [==============================] - 0s 867us/step - loss: 0.7322 - acc: 0.8446\n",
      "Epoch 71/100\n",
      "399/399 [==============================] - 0s 903us/step - loss: 0.7284 - acc: 0.8271\n",
      "Epoch 72/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.7193 - acc: 0.8471\n",
      "Epoch 73/100\n",
      "399/399 [==============================] - 0s 850us/step - loss: 0.6854 - acc: 0.8346\n",
      "Epoch 74/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.6594 - acc: 0.8571\n",
      "Epoch 75/100\n",
      "399/399 [==============================] - 0s 837us/step - loss: 0.6748 - acc: 0.8521\n",
      "Epoch 76/100\n",
      "399/399 [==============================] - 0s 769us/step - loss: 0.6315 - acc: 0.8596\n",
      "Epoch 77/100\n",
      "399/399 [==============================] - 0s 792us/step - loss: 0.6024 - acc: 0.8672\n",
      "Epoch 78/100\n",
      "399/399 [==============================] - 0s 923us/step - loss: 0.5928 - acc: 0.8747\n",
      "Epoch 79/100\n",
      "399/399 [==============================] - 0s 952us/step - loss: 0.5450 - acc: 0.8797\n",
      "Epoch 80/100\n",
      "399/399 [==============================] - 0s 870us/step - loss: 0.5513 - acc: 0.8697\n",
      "Epoch 81/100\n",
      "399/399 [==============================] - 0s 780us/step - loss: 0.5212 - acc: 0.8972\n",
      "Epoch 82/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.4937 - acc: 0.9223\n",
      "Epoch 83/100\n",
      "399/399 [==============================] - 0s 995us/step - loss: 0.5148 - acc: 0.8922\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 0s 931us/step - loss: 0.4938 - acc: 0.8872\n",
      "Epoch 85/100\n",
      "399/399 [==============================] - 0s 890us/step - loss: 0.4733 - acc: 0.8997\n",
      "Epoch 86/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.4327 - acc: 0.9223\n",
      "Epoch 87/100\n",
      "399/399 [==============================] - 0s 931us/step - loss: 0.4248 - acc: 0.9348\n",
      "Epoch 88/100\n",
      "399/399 [==============================] - 0s 951us/step - loss: 0.4062 - acc: 0.9248\n",
      "Epoch 89/100\n",
      "399/399 [==============================] - 0s 835us/step - loss: 0.4169 - acc: 0.9223\n",
      "Epoch 90/100\n",
      "399/399 [==============================] - 0s 823us/step - loss: 0.3448 - acc: 0.9599\n",
      "Epoch 91/100\n",
      "399/399 [==============================] - 0s 1ms/step - loss: 0.3387 - acc: 0.9499\n",
      "Epoch 92/100\n",
      "399/399 [==============================] - 0s 861us/step - loss: 0.3074 - acc: 0.9599\n",
      "Epoch 93/100\n",
      "399/399 [==============================] - 0s 711us/step - loss: 0.3458 - acc: 0.9348\n",
      "Epoch 94/100\n",
      "399/399 [==============================] - 0s 889us/step - loss: 0.3238 - acc: 0.9599\n",
      "Epoch 95/100\n",
      "399/399 [==============================] - 0s 927us/step - loss: 0.3020 - acc: 0.9624\n",
      "Epoch 96/100\n",
      "399/399 [==============================] - 0s 833us/step - loss: 0.2820 - acc: 0.9524\n",
      "Epoch 97/100\n",
      "399/399 [==============================] - 0s 801us/step - loss: 0.3058 - acc: 0.9649\n",
      "Epoch 98/100\n",
      "399/399 [==============================] - 0s 838us/step - loss: 0.2622 - acc: 0.9649\n",
      "Epoch 99/100\n",
      "399/399 [==============================] - 0s 820us/step - loss: 0.2786 - acc: 0.9674\n",
      "Epoch 100/100\n",
      "399/399 [==============================] - 0s 912us/step - loss: 0.2662 - acc: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x111ee3a58>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpointer = ModelCheckpoint(filepath=bestSavedModel, verbose=1, save_best_only=True)\n",
    "#model.fit(X, y, epochs=epochs, batch_size=batchSize, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizerFunction, metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save(bestSavedModel)\n",
    "\n",
    "# save the vocabulary map. We need it for character generation part later.\n",
    "dump(vocabMap, open(mappingFileName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the generated character sequence\n",
    "generatedCharSeqLength = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generateSequence(model, reverseVocab, seedRawText, length):\n",
    "    inputText = seedRawText\n",
    "    \n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(length):\n",
    "        \n",
    "        # the seed text needs to be processed just like the training text was.\n",
    "\n",
    "        # encode the characters as integers based on the dictionary\n",
    "        encodedSeedText = [vocabMap[c] for c in inputText]\n",
    "\n",
    "        # truncate sequences to a fixed length using Keras' pad_sequence()\n",
    "        encodedSeedText = pad_sequences([encodedSeedText], maxlen=sequenceLength, truncating='pre')\n",
    "\n",
    "        # one-hot encode\n",
    "        oneHotEncodedSeedText = to_categorical(encodedSeedText, num_classes=vocabSize)\n",
    "        \n",
    "        # use the model to predict character\n",
    "        predCharInt = model.predict_classes(oneHotEncodedSeedText, verbose=0)\n",
    "        predChar = reverseVocab[predCharInt[0]]\n",
    "\n",
    "        # append to input\n",
    "        inputText += predChar\n",
    "        \n",
    "    return inputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, ',': 2, '.': 3, ';': 4, 'A': 5, 'B': 6, 'C': 7, 'E': 8, 'F': 9, 'H': 10, 'S': 11, 'T': 12, 'W': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'w': 34, 'x': 35, 'y': 36, '’': 37}\n",
      "{0: '\\n', 1: ' ', 2: ',', 3: '.', 4: ';', 5: 'A', 6: 'B', 7: 'C', 8: 'E', 9: 'F', 10: 'H', 11: 'S', 12: 'T', 13: 'W', 14: 'a', 15: 'b', 16: 'c', 17: 'd', 18: 'e', 19: 'f', 20: 'g', 21: 'h', 22: 'i', 23: 'k', 24: 'l', 25: 'm', 26: 'n', 27: 'o', 28: 'p', 29: 'q', 30: 'r', 31: 's', 32: 't', 33: 'u', 34: 'w', 35: 'x', 36: 'y', 37: '’'}\n"
     ]
    }
   ],
   "source": [
    "# load the model and the text dictionary\n",
    "model.load_weights(bestSavedModel)\n",
    "\n",
    "# vocabMap is a char: int dictionary\n",
    "vocabMap = load(open(mappingFileName, 'rb'))\n",
    "\n",
    "# reverseVocab is a int: char dictionary used to convert the model prediction (int) to char\n",
    "reverseVocab = dict(enumerate(vocabMap))\n",
    "\n",
    "print(vocabMap)\n",
    "print(reverseVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the seed text file (input text)\n",
    "seedRawText = readTextFile(seedTextFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generatedText = generateSequence(model, reverseVocab, seedRawText, generatedCharSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasn’t that a dainty dish queen was in the parl d in a pie.\n",
      " hen ing he  eieey. Th  piid . heeen ye  The boree\n"
     ]
    }
   ],
   "source": [
    "print(generatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
