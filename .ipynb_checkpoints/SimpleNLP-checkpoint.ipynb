{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna-short.txt\n",
      "anna-short-Train.txt\n",
      "anna-short-SimpleNLPMapping.pkl\n",
      "anna-short-SimpleNLPModel.hdf5\n",
      "anna-short-SeedText.txt\n"
     ]
    }
   ],
   "source": [
    "sequenceLength = 10\n",
    "\n",
    "#sourceTextFileName = \"poem.txt\"\n",
    "sourceTextFileName = \"anna-short.txt\"\n",
    "trainingTextFileName = sourceTextFileName.split(\".\")[0] + \"-Train.txt\"\n",
    "mappingFileName = sourceTextFileName.split(\".\")[0] + \"-SimpleNLPMapping\" + \".pkl\"\n",
    "bestSavedModel = sourceTextFileName.split(\".\")[0] + \"-SimpleNLPModel\" + \".hdf5\"\n",
    "seedTextFileName = sourceTextFileName.split(\".\")[0] + \"-SeedText.txt\"\n",
    "\n",
    "print(sourceTextFileName)\n",
    "print(trainingTextFileName)\n",
    "print(mappingFileName)\n",
    "print(bestSavedModel)\n",
    "print(seedTextFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads and return the text of the training file. self-explanatory\n",
    "def readTextFile(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTextFile(lines, fileName):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(fileName, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRawText(rawText):\n",
    "    # split the raw text using space (' ') \n",
    "    tokens = rawText.split()\n",
    "    rawText = ' '.join(tokens)\n",
    "    \n",
    "    # basically we removed all the line/paragraph breaks\n",
    "    # but kept the punctuations\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequenceRawText(rawText):\n",
    "    # organize into sequences of characters\n",
    "    length = 10\n",
    "    sequences = list()\n",
    "    for i in range(length, len(rawText)):\n",
    "        # picks a sequence of tokens\n",
    "        seq = rawText[i - length:i+1]\n",
    "        # add to tlist\n",
    "        sequences.append(seq)\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everything was in confusion in the Oblonskys' hou\n"
     ]
    }
   ],
   "source": [
    "# load the training file\n",
    "rawText = readTextFile(sourceTextFileName)\n",
    "print(rawText[: 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys' house.\n"
     ]
    }
   ],
   "source": [
    "rawText = processRawText(rawText)\n",
    "print(rawText[: 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences: 21865\n",
      "['Chapter 1 H', 'hapter 1 Ha', 'apter 1 Hap', 'pter 1 Happ', 'ter 1 Happy']\n"
     ]
    }
   ],
   "source": [
    "# turn the text into a sequence of character\n",
    "# each sequence is sequenceLength long\n",
    "sequences = sequenceRawText(rawText)\n",
    "\n",
    "# save sequences to file\n",
    "writeTextFile(sequences, trainingTextFileName)\n",
    "\n",
    "print('Total number of sequences: %d' % len(sequences))\n",
    "print(sequences[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter 1 H', 'hapter 1 Ha', 'apter 1 Hap', 'pter 1 Happ', 'ter 1 Happy']\n"
     ]
    }
   ],
   "source": [
    "# now we read the text sequences from the file we saved\n",
    "rawTrainingText = readTextFile(trainingTextFileName)\n",
    "sequenceLines = rawTrainingText.split('\\n')\n",
    "\n",
    "print(sequenceLines[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 63\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '1', '2', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'W', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '1': 10, '2': 11, '3': 12, ':': 13, ';': 14, '?': 15, 'A': 16, 'B': 17, 'C': 18, 'D': 19, 'E': 20, 'F': 21, 'G': 22, 'H': 23, 'I': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'O': 29, 'P': 30, 'R': 31, 'S': 32, 'T': 33, 'W': 34, 'Y': 35, '_': 36, 'a': 37, 'b': 38, 'c': 39, 'd': 40, 'e': 41, 'f': 42, 'g': 43, 'h': 44, 'i': 45, 'j': 46, 'k': 47, 'l': 48, 'm': 49, 'n': 50, 'o': 51, 'p': 52, 'q': 53, 'r': 54, 's': 55, 't': 56, 'u': 57, 'v': 58, 'w': 59, 'x': 60, 'y': 61, 'z': 62}\n"
     ]
    }
   ],
   "source": [
    "# time to encode the text\n",
    "\n",
    "# create a vocabulary (all used characters in the text)\n",
    "vocab = sorted(list(set(rawTrainingText)))\n",
    "vocabSize = len(vocab)\n",
    "\n",
    "# map each character to an integer by creating a dictionary\n",
    "vocabMap = dict((c, i) for i, c in enumerate(vocab))\n",
    "\n",
    "print(\"Vocabulary size: %d\" % vocabSize)\n",
    "print(vocab)\n",
    "print(vocabMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 44, 37, 52, 56, 41, 54, 1, 10, 1, 23], [44, 37, 52, 56, 41, 54, 1, 10, 1, 23, 37], [37, 52, 56, 41, 54, 1, 10, 1, 23, 37, 52], [52, 56, 41, 54, 1, 10, 1, 23, 37, 52, 52], [56, 41, 54, 1, 10, 1, 23, 37, 52, 52, 61]]\n"
     ]
    }
   ],
   "source": [
    "# let's turn those characters in the sequences to integers \n",
    "sequences = list()\n",
    "\n",
    "for line in sequenceLines:\n",
    "    # encode line\n",
    "    encodedSequence = [vocabMap[char] for char in line]\n",
    "    # add it to the list\n",
    "    sequences.append(encodedSequence)\n",
    "    \n",
    "print(sequences[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 44 37 52 56 41 54  1 10  1]\n",
      " [44 37 52 56 41 54  1 10  1 23]\n",
      " [37 52 56 41 54  1 10  1 23 37]\n",
      " [52 56 41 54  1 10  1 23 37 52]\n",
      " [56 41 54  1 10  1 23 37 52 52]]\n",
      "[23 37 52 52 61]\n"
     ]
    }
   ],
   "source": [
    "# now we need to prepare the input and target matrices\n",
    "\n",
    "# X = sequences[:,:-1] means we are grabbing all the rows from sequences but dropping the last column\n",
    "# .... the last column will be used as the target\n",
    "\n",
    "# y = sequences[:,-1] means we are grabbing all the rows from sequences but only retaining the last column \n",
    "# .... the last column being our target\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last thing...\n",
    "# we need to one-hot-encode each character. \n",
    "# That is, each input vector (of sequenceLength) becomes a vector as long as the vocabulary\n",
    "# with a 1 marked for the specific character. \n",
    "\n",
    "# for this we use the to_categorical() function in the Keras API to one-hot-encode\n",
    "\n",
    "sequences = [to_categorical(x, num_classes=vocabSize) for x in X]\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(y, num_classes=vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially each input becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, sequenceLength, vocabSize)\n",
    "\n",
    "# and each output becomes a matrice with the following dimensions:\n",
    "# (number_of_sequences, vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21865, 10, 63)\n",
      "(21865, 63)\n",
      "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batchSize = 20\n",
    "dropOutRate = 0.3\n",
    "lstmCellsNumber = 100\n",
    "\n",
    "activationFunction = \"softmax\"\n",
    "\n",
    "optimizerFunction = \"adam\"\n",
    "# optimizerFunction = \"rmsprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               65600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 63)                6363      \n",
      "=================================================================\n",
      "Total params: 71,963\n",
      "Trainable params: 71,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# time to model...\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstmCellsNumber, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(dropOutRate))\n",
    "model.add(Dense(vocabSize, activation=activationFunction))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21865/21865 [==============================] - 16s 715us/step - loss: 2.9735 - acc: 0.2051\n",
      "Epoch 2/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 2.5323 - acc: 0.2961\n",
      "Epoch 3/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 2.3576 - acc: 0.3342\n",
      "Epoch 4/100\n",
      "21865/21865 [==============================] - 15s 675us/step - loss: 2.2511 - acc: 0.3660\n",
      "Epoch 5/100\n",
      "21865/21865 [==============================] - 14s 656us/step - loss: 2.1687 - acc: 0.3841\n",
      "Epoch 6/100\n",
      "21865/21865 [==============================] - 15s 670us/step - loss: 2.0936 - acc: 0.3998\n",
      "Epoch 7/100\n",
      "21865/21865 [==============================] - 14s 638us/step - loss: 2.0470 - acc: 0.4139\n",
      "Epoch 8/100\n",
      "21865/21865 [==============================] - 14s 652us/step - loss: 1.9914 - acc: 0.4255\n",
      "Epoch 9/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 1.9485 - acc: 0.4373\n",
      "Epoch 10/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 1.9051 - acc: 0.4436\n",
      "Epoch 11/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 1.8736 - acc: 0.4547\n",
      "Epoch 12/100\n",
      "21865/21865 [==============================] - 15s 665us/step - loss: 1.8333 - acc: 0.4654\n",
      "Epoch 13/100\n",
      "21865/21865 [==============================] - 15s 677us/step - loss: 1.8003 - acc: 0.4715\n",
      "Epoch 14/100\n",
      "21865/21865 [==============================] - 15s 675us/step - loss: 1.7602 - acc: 0.4834\n",
      "Epoch 15/100\n",
      "21865/21865 [==============================] - 15s 675us/step - loss: 1.7280 - acc: 0.4900\n",
      "Epoch 16/100\n",
      "21865/21865 [==============================] - 14s 659us/step - loss: 1.6965 - acc: 0.5003\n",
      "Epoch 17/100\n",
      "21865/21865 [==============================] - 14s 643us/step - loss: 1.6641 - acc: 0.5070\n",
      "Epoch 18/100\n",
      "21865/21865 [==============================] - 15s 679us/step - loss: 1.6353 - acc: 0.5121\n",
      "Epoch 19/100\n",
      "21865/21865 [==============================] - 15s 688us/step - loss: 1.5996 - acc: 0.5233\n",
      "Epoch 20/100\n",
      "21865/21865 [==============================] - 14s 657us/step - loss: 1.5681 - acc: 0.5326\n",
      "Epoch 21/100\n",
      "21865/21865 [==============================] - 14s 650us/step - loss: 1.5479 - acc: 0.5352\n",
      "Epoch 22/100\n",
      "21865/21865 [==============================] - 14s 655us/step - loss: 1.5172 - acc: 0.5443\n",
      "Epoch 23/100\n",
      "21865/21865 [==============================] - 15s 664us/step - loss: 1.4918 - acc: 0.5520\n",
      "Epoch 24/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 1.4680 - acc: 0.5603\n",
      "Epoch 25/100\n",
      "21865/21865 [==============================] - 14s 651us/step - loss: 1.4435 - acc: 0.5662\n",
      "Epoch 26/100\n",
      "21865/21865 [==============================] - 15s 681us/step - loss: 1.4137 - acc: 0.5731\n",
      "Epoch 27/100\n",
      "21865/21865 [==============================] - 14s 655us/step - loss: 1.3904 - acc: 0.5769\n",
      "Epoch 28/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 1.3639 - acc: 0.5845\n",
      "Epoch 29/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 1.3330 - acc: 0.5929\n",
      "Epoch 30/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 1.3141 - acc: 0.5975\n",
      "Epoch 31/100\n",
      "21865/21865 [==============================] - 14s 649us/step - loss: 1.2906 - acc: 0.6054\n",
      "Epoch 32/100\n",
      "21865/21865 [==============================] - 15s 673us/step - loss: 1.2674 - acc: 0.6136\n",
      "Epoch 33/100\n",
      "21865/21865 [==============================] - 14s 661us/step - loss: 1.2522 - acc: 0.6167\n",
      "Epoch 34/100\n",
      "21865/21865 [==============================] - 14s 633us/step - loss: 1.2241 - acc: 0.6244\n",
      "Epoch 35/100\n",
      "21865/21865 [==============================] - 14s 651us/step - loss: 1.2038 - acc: 0.6304\n",
      "Epoch 36/100\n",
      "21865/21865 [==============================] - 16s 722us/step - loss: 1.1781 - acc: 0.6345\n",
      "Epoch 37/100\n",
      "21865/21865 [==============================] - 15s 663us/step - loss: 1.1535 - acc: 0.6438\n",
      "Epoch 38/100\n",
      "21865/21865 [==============================] - 14s 658us/step - loss: 1.1445 - acc: 0.6428\n",
      "Epoch 39/100\n",
      "21865/21865 [==============================] - 14s 663us/step - loss: 1.1162 - acc: 0.6518\n",
      "Epoch 40/100\n",
      "21865/21865 [==============================] - 14s 661us/step - loss: 1.0973 - acc: 0.6592\n",
      "Epoch 41/100\n",
      "21865/21865 [==============================] - 15s 669us/step - loss: 1.0740 - acc: 0.6665\n",
      "Epoch 42/100\n",
      "21865/21865 [==============================] - 15s 668us/step - loss: 1.0544 - acc: 0.6690\n",
      "Epoch 43/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 1.0574 - acc: 0.6698\n",
      "Epoch 44/100\n",
      "21865/21865 [==============================] - 14s 661us/step - loss: 1.0136 - acc: 0.6796\n",
      "Epoch 45/100\n",
      "21865/21865 [==============================] - 14s 657us/step - loss: 1.0024 - acc: 0.6841\n",
      "Epoch 46/100\n",
      "21865/21865 [==============================] - 15s 663us/step - loss: 0.9949 - acc: 0.6846\n",
      "Epoch 47/100\n",
      "21865/21865 [==============================] - 14s 649us/step - loss: 0.9625 - acc: 0.6988\n",
      "Epoch 48/100\n",
      "21865/21865 [==============================] - 14s 647us/step - loss: 0.9561 - acc: 0.6972\n",
      "Epoch 49/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 0.9445 - acc: 0.7022\n",
      "Epoch 50/100\n",
      "21865/21865 [==============================] - 14s 654us/step - loss: 0.9216 - acc: 0.7067\n",
      "Epoch 51/100\n",
      "21865/21865 [==============================] - 14s 661us/step - loss: 0.9054 - acc: 0.7136\n",
      "Epoch 52/100\n",
      "21865/21865 [==============================] - 15s 672us/step - loss: 0.8995 - acc: 0.7136\n",
      "Epoch 53/100\n",
      "21865/21865 [==============================] - 15s 665us/step - loss: 0.8839 - acc: 0.7176\n",
      "Epoch 54/100\n",
      "21865/21865 [==============================] - 15s 671us/step - loss: 0.8650 - acc: 0.7229\n",
      "Epoch 55/100\n",
      "21865/21865 [==============================] - 14s 643us/step - loss: 0.8549 - acc: 0.7264\n",
      "Epoch 56/100\n",
      "21865/21865 [==============================] - 14s 649us/step - loss: 0.8369 - acc: 0.7329\n",
      "Epoch 57/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 0.8195 - acc: 0.7375\n",
      "Epoch 58/100\n",
      "21865/21865 [==============================] - 14s 661us/step - loss: 0.8133 - acc: 0.7394\n",
      "Epoch 59/100\n",
      "21865/21865 [==============================] - 15s 707us/step - loss: 0.8121 - acc: 0.7370\n",
      "Epoch 60/100\n",
      "21865/21865 [==============================] - 16s 711us/step - loss: 0.7929 - acc: 0.7437\n",
      "Epoch 61/100\n",
      "21865/21865 [==============================] - 15s 681us/step - loss: 0.7708 - acc: 0.7485\n",
      "Epoch 62/100\n",
      "21865/21865 [==============================] - 15s 677us/step - loss: 0.7760 - acc: 0.7525\n",
      "Epoch 63/100\n",
      "21865/21865 [==============================] - 16s 718us/step - loss: 0.7485 - acc: 0.7603\n",
      "Epoch 64/100\n",
      "21865/21865 [==============================] - 15s 677us/step - loss: 0.7539 - acc: 0.7581\n",
      "Epoch 65/100\n",
      "21865/21865 [==============================] - 14s 662us/step - loss: 0.7308 - acc: 0.7606\n",
      "Epoch 66/100\n",
      "21865/21865 [==============================] - 15s 678us/step - loss: 0.7317 - acc: 0.7641\n",
      "Epoch 67/100\n",
      "21865/21865 [==============================] - 15s 688us/step - loss: 0.7240 - acc: 0.7637\n",
      "Epoch 68/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 0.7107 - acc: 0.7705\n",
      "Epoch 69/100\n",
      "21865/21865 [==============================] - 16s 719us/step - loss: 0.6911 - acc: 0.7765\n",
      "Epoch 70/100\n",
      "21865/21865 [==============================] - 15s 669us/step - loss: 0.6860 - acc: 0.7751\n",
      "Epoch 71/100\n",
      "21865/21865 [==============================] - 14s 658us/step - loss: 0.6806 - acc: 0.7803\n",
      "Epoch 72/100\n",
      "21865/21865 [==============================] - 15s 664us/step - loss: 0.6716 - acc: 0.7812\n",
      "Epoch 73/100\n",
      "21865/21865 [==============================] - 14s 647us/step - loss: 0.6602 - acc: 0.7831\n",
      "Epoch 74/100\n",
      "21865/21865 [==============================] - 14s 646us/step - loss: 0.6545 - acc: 0.7886\n",
      "Epoch 75/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 0.6516 - acc: 0.7878\n",
      "Epoch 76/100\n",
      "21865/21865 [==============================] - 14s 659us/step - loss: 0.6490 - acc: 0.7878\n",
      "Epoch 77/100\n",
      "21865/21865 [==============================] - 15s 665us/step - loss: 0.6321 - acc: 0.7945\n",
      "Epoch 78/100\n",
      "21865/21865 [==============================] - 15s 665us/step - loss: 0.6353 - acc: 0.7938\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21865/21865 [==============================] - 14s 661us/step - loss: 0.6202 - acc: 0.7970\n",
      "Epoch 80/100\n",
      "21865/21865 [==============================] - 15s 680us/step - loss: 0.6126 - acc: 0.8003\n",
      "Epoch 81/100\n",
      "21865/21865 [==============================] - 14s 656us/step - loss: 0.6123 - acc: 0.8036\n",
      "Epoch 82/100\n",
      "21865/21865 [==============================] - 14s 658us/step - loss: 0.6066 - acc: 0.8025\n",
      "Epoch 83/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 0.6063 - acc: 0.8006\n",
      "Epoch 84/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 0.5976 - acc: 0.8038\n",
      "Epoch 85/100\n",
      "21865/21865 [==============================] - 14s 656us/step - loss: 0.5836 - acc: 0.8109\n",
      "Epoch 86/100\n",
      "21865/21865 [==============================] - 14s 659us/step - loss: 0.5895 - acc: 0.8085\n",
      "Epoch 87/100\n",
      "21865/21865 [==============================] - 15s 666us/step - loss: 0.5850 - acc: 0.8072\n",
      "Epoch 88/100\n",
      "21865/21865 [==============================] - 15s 678us/step - loss: 0.5695 - acc: 0.8115\n",
      "Epoch 89/100\n",
      "21865/21865 [==============================] - 15s 670us/step - loss: 0.5696 - acc: 0.8117\n",
      "Epoch 90/100\n",
      "21865/21865 [==============================] - 15s 670us/step - loss: 0.5663 - acc: 0.8129\n",
      "Epoch 91/100\n",
      "21865/21865 [==============================] - 15s 677us/step - loss: 0.5573 - acc: 0.8163\n",
      "Epoch 92/100\n",
      "21865/21865 [==============================] - 14s 659us/step - loss: 0.5474 - acc: 0.8181\n",
      "Epoch 93/100\n",
      "21865/21865 [==============================] - 96s 4ms/step - loss: 0.5520 - acc: 0.8188\n",
      "Epoch 94/100\n",
      "21865/21865 [==============================] - 15s 690us/step - loss: 0.5394 - acc: 0.8230\n",
      "Epoch 95/100\n",
      "21865/21865 [==============================] - 14s 653us/step - loss: 0.5317 - acc: 0.8250\n",
      "Epoch 96/100\n",
      "21865/21865 [==============================] - 15s 663us/step - loss: 0.5261 - acc: 0.8293\n",
      "Epoch 97/100\n",
      "21865/21865 [==============================] - 15s 671us/step - loss: 0.5335 - acc: 0.8240\n",
      "Epoch 98/100\n",
      "21865/21865 [==============================] - 16s 740us/step - loss: 0.5233 - acc: 0.8273\n",
      "Epoch 99/100\n",
      "21865/21865 [==============================] - 14s 663us/step - loss: 0.5202 - acc: 0.8300\n",
      "Epoch 100/100\n",
      "21865/21865 [==============================] - 14s 654us/step - loss: 0.5127 - acc: 0.8306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118a83748>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpointer = ModelCheckpoint(filepath=bestSavedModel, verbose=1, save_best_only=True)\n",
    "#model.fit(X, y, epochs=epochs, batch_size=batchSize, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizerFunction, metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save(bestSavedModel)\n",
    "\n",
    "# save the vocabulary map. We need it for character generation part later.\n",
    "dump(vocabMap, open(mappingFileName, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the generated character sequence\n",
    "generatedCharSeqLength = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generateSequence(model, reverseVocab, seedRawText, length):\n",
    "    inputText = seedRawText\n",
    "    \n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(length):\n",
    "        \n",
    "        # the seed text needs to be processed just like the training text was.\n",
    "\n",
    "        # encode the characters as integers based on the dictionary\n",
    "        encodedSeedText = [vocabMap[c] for c in inputText]\n",
    "\n",
    "        # truncate sequences to a fixed length using Keras' pad_sequence()\n",
    "        encodedSeedText = pad_sequences([encodedSeedText], maxlen=sequenceLength, truncating='pre')\n",
    "\n",
    "        # one-hot encode\n",
    "        oneHotEncodedSeedText = to_categorical(encodedSeedText, num_classes=vocabSize)\n",
    "        \n",
    "        # use the model to predict character\n",
    "        predCharInt = model.predict_classes(oneHotEncodedSeedText, verbose=0)\n",
    "        predChar = reverseVocab[predCharInt[0]]\n",
    "\n",
    "        # append to input\n",
    "        inputText += predChar\n",
    "        \n",
    "    return inputText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '1': 10, '2': 11, '3': 12, ':': 13, ';': 14, '?': 15, 'A': 16, 'B': 17, 'C': 18, 'D': 19, 'E': 20, 'F': 21, 'G': 22, 'H': 23, 'I': 24, 'K': 25, 'L': 26, 'M': 27, 'N': 28, 'O': 29, 'P': 30, 'R': 31, 'S': 32, 'T': 33, 'W': 34, 'Y': 35, '_': 36, 'a': 37, 'b': 38, 'c': 39, 'd': 40, 'e': 41, 'f': 42, 'g': 43, 'h': 44, 'i': 45, 'j': 46, 'k': 47, 'l': 48, 'm': 49, 'n': 50, 'o': 51, 'p': 52, 'q': 53, 'r': 54, 's': 55, 't': 56, 'u': 57, 'v': 58, 'w': 59, 'x': 60, 'y': 61, 'z': 62}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '1', 11: '2', 12: '3', 13: ':', 14: ';', 15: '?', 16: 'A', 17: 'B', 18: 'C', 19: 'D', 20: 'E', 21: 'F', 22: 'G', 23: 'H', 24: 'I', 25: 'K', 26: 'L', 27: 'M', 28: 'N', 29: 'O', 30: 'P', 31: 'R', 32: 'S', 33: 'T', 34: 'W', 35: 'Y', 36: '_', 37: 'a', 38: 'b', 39: 'c', 40: 'd', 41: 'e', 42: 'f', 43: 'g', 44: 'h', 45: 'i', 46: 'j', 47: 'k', 48: 'l', 49: 'm', 50: 'n', 51: 'o', 52: 'p', 53: 'q', 54: 'r', 55: 's', 56: 't', 57: 'u', 58: 'v', 59: 'w', 60: 'x', 61: 'y', 62: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# load the model and the text dictionary\n",
    "model.load_weights(bestSavedModel)\n",
    "\n",
    "# vocabMap is a char: int dictionary\n",
    "vocabMap = load(open(mappingFileName, 'rb'))\n",
    "\n",
    "# reverseVocab is a int: char dictionary used to convert the model prediction (int) to char\n",
    "reverseVocab = dict(enumerate(vocabMap))\n",
    "\n",
    "print(vocabMap)\n",
    "print(reverseVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the seed text file (input text)\n",
    "seedRawText = readTextFile(seedTextFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generatedText = generateSequence(model, reverseVocab, seedRawText, generatedCharSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that to amend, to set right their relations was impossible,\n",
      "because it was impossible to make her attractive again and able to\n",
      "inspire love, or to make him an old man, not susceptible to love. Except\n",
      "deceit and lying nothing could come of it now; and deceit and lying were\n",
      "opposed to his nature.\n",
      "\" she tiok, or tereselles with he wifl comsed the door, he called the mono insold too himainace of anre. \"Well might be of use to her fair; hus selfiess, and that it was quite senseless in our day in whach he was prateed when him arding in alowar, and to he mabed to him at that instadith on his side. \"Well, what alstay, paisend on ahd was alceady wat excing and tor that havingountrend on his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And thereope, and his handsome face as untersing on excell newander to goine he tailed the herrelestion with his wife was not sleeping in his wife's bedroom. And there\n"
     ]
    }
   ],
   "source": [
    "print(generatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
